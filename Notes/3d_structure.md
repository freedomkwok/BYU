
napari
çƒ­å›¾é™ä½8å€
Gaussian Heatmap

âœ… åå·ç§¯ï¼ˆTransposed Convolutionï¼‰
  ç”¨äºâ€œæ”¾å¤§â€ç‰¹å¾å›¾
  ä¾‹å¦‚ï¼š[8 Ã— 8] â†’ åå·ç§¯ â†’ [16 Ã— 16]
  åœ¨ å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANï¼‰ä¸­å¾ˆå¸¸è§

âœ… RNNï¼ˆRecurrent Neural Networkï¼‰
  ç”¨äºå»ºæ¨¡ æ—¶é—´/åºåˆ—ä¾èµ–æ€§
  ä¾‹å¦‚ï¼šè¾“å…¥ä¸€æ®µå¥å­ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚



ğŸ§  How does it work?
1. Label creation:
    You generate a 3D Gaussian heatmap from (cx, cy, cz) â€” this becomes your supervised ground truth Y_true, shaped like [D, H, W].

H(x, y) = exp(-((x - Î¼x)Â² + (y - Î¼y)Â²) / (2ÏƒÂ²))
2. Model output:
    Your model outputs a prediction Y_pred, also shaped like [D, H, W], which tries to mimic the heatmap â€” ideally peaking at the same spot.

3. Loss function:
oss Type
What it does
MSELoss (L2)
Penalizes pixel-wise squared difference:
loss = (Y_pred - Y_true)^2

1 You generate a target heatmap from (cx, cy, cz) with a Gaussian
2 Your model outputs a predicted heatmap
3 You use MSE or similar loss to measure:
    how close is prediction to ground truth at every voxel?
4 The model learns to put its own peak near the real center
5 At inference, you can use argmax() to find the predicted center
6 You can compare pred_center vs true_center for localization error

SmoothBCE  Binary Cross Entropy Loss (BCE) is used for binary classification
Standard BCE formula:
\text{BCE}(p, y) = -[y \cdot \log(p) + (1 - y) \cdot \log(1 - p)]
Where:
	â€¢	p is the predicted probability (from sigmoid)
	â€¢	y is the ground truth (0 or 1)

def smooth_bce(eps=0.1):
    return 1.0 - 0.5 * eps, 0.5 * eps  # positive_label, negative_label
pos, neg = smooth_bce(0.1)  # pos=0.95, neg=0.05

ğŸ§Š What is Smooth BCE (or Label Smoothing for BCE)?
ğŸ“Œ Problem:
When training with hard 0 or 1 labels, the model can become overconfident, leading to overfitting or poor generalization.

âœ… Solution:
Label smoothing replaces the hard labels 0 and 1 with softer values like 0.05 and 0.95.