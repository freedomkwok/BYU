{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# BYU Locating Flagellar Motors\n",
    "\n",
    "## YOLO Model Training Notebook\n",
    "\n",
    "This is the third notebook in a series for the BYU Locating Bacterial Flagellar Motors 2025 Kaggle challenge. This notebook handles the training of YOLOv8 object detection models on our prepared dataset.\n",
    "\n",
    "### Notebook Series:\n",
    "1. **[Parse Data](https://www.kaggle.com/code/andrewjdarley/parse-data)**: Extracting and preparing 2D slices containing motors to make a YOLO dataset\n",
    "2. **[Visualize Data](https://www.kaggle.com/code/andrewjdarley/visualize-data)**: Exploratory data analysis and visualization of annotated motor locations\n",
    "3. **Train YOLO (Current)**: Fine tuning an YOLOv8 object detection model on the prepared dataset\n",
    "4. **[Submission Notebook](https://www.kaggle.com/code/andrewjdarley/submission-notebook)**: Running inference and generating submission files \n",
    "\n",
    "## About this Notebook\n",
    "\n",
    "This training notebook implements a full YOLOv8 training pipeline for detecting bacterial flagellar motors in tomographic slices. The notebook:\n",
    "\n",
    "1. **Dataset Configuration**: Sets up and validates the YOLO-format dataset YAML configuration\n",
    "2. **Model Initialization**: Loads pre-trained YOLOv8 weights for transfer learning\n",
    "3. **Training Process**: Fine tunes the model with early stopping and periodic checkpoints\n",
    "4. **Loss Visualization**: Plots training and validation dfl loss curves to monitor progress\n",
    "5. **Performance Evaluation**: Tests the trained model on random validation samples\n",
    "6. **Model Export**: Saves the trained weights for use in the submission notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Layer | Description        | Local Stride | Cumulative Stride | Output Size |\n",
    "# | ----- | ------------------ | ------------ | ----------------- | ----------- |\n",
    "# | Input | Original image     | -            | 1×                | `640 × 640` |\n",
    "# | 0     | `Conv(64, 3, 2)`   | 2            | 2×                | `320 × 320` |\n",
    "# | 1     | `Conv(128, 3, 2)`  | 2            | 4× (2×2)          | `160 × 160` |\n",
    "# | 2     | `C3`               | 1            | 4×                | `160 × 160` |\n",
    "# | 3     | `Conv(256, 3, 2)`  | 2            | 8× (4×2)          | `80 × 80`   |\n",
    "# | 5     | `Conv(512, 3, 2)`  | 2            | 16× (8×2)         | `40 × 40`   |\n",
    "# | 7     | `Conv(1024, 3, 2)` | 2            | 32× (16×2)        | `20 × 20`   |\n",
    "\n",
    "\n",
    "# | Layer | Operation       | Stride | Output Size           | Note         |\n",
    "# | ----- | --------------- | ------ | --------------------- | ------------ |\n",
    "# | 0     | Conv(64, s=2)   | 2×     | 320 × 320             | P1/2         |\n",
    "# | 1     | Conv(128, s=2)  | 4×     | 160 × 160             | P2/4         |\n",
    "# | 2     | C3              | 4×     | 160 × 160 (no stride) |              |\n",
    "# | 3     | Conv(256, s=2)  | 8×     | 80 × 80               | **P3/8** ✅   |\n",
    "# | 4     | C3              | 8×     | 80 × 80               |              |\n",
    "# | 5     | Conv(512, s=2)  | 16×    | 40 × 40               | **P4/16** ✅  |\n",
    "# | 6     | C3              | 16×    | 40 × 40               |              |\n",
    "# | 7     | Conv(1024, s=2) | 32×    | 20 × 20               | **P5/32** ✅  |\n",
    "# | 8–10  | etc             | 32×    | 20 × 20               | refined head |\n",
    "\n",
    "# | Layer | Output Stride | Role  | Connect to YOLO Head   |\n",
    "# | ----- | ------------- | ----- | ---------------------- |\n",
    "# | 3     | 8×            | P3/8  | ✅ Yes (small objects)  |\n",
    "# | 5     | 16×           | P4/16 | ✅ Yes (medium objects) |\n",
    "# | 7     | 32×           | P5/32 | ✅ Yes (large objects)  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Backbone            | Params (M) | FLOPs (GFLOPs) | Notes                                                         |\n",
    "# | ------------------- | ---------- | -------------- | ------------------------------------------------------------- |\n",
    "# | **CSPDarknet**      | 7–70M      | Low–High       | Native YOLO backbone (used in YOLOv4, YOLOv5)                 |\n",
    "# | **ResNet-18**       | \\~11.7M    | \\~1.8          | Shallow, efficient; good for small datasets or edge inference |\n",
    "# | **ResNet-34**       | \\~21.8M    | \\~3.6          | Balanced choice if ResNet-18 underperforms                    |\n",
    "# | **ResNet-50**       | \\~25.6M    | \\~4.1          | Common backbone for higher accuracy (heavier)                 |\n",
    "# | **MobileNetV2**     | \\~3.4M     | \\~0.3          | Extremely lightweight; great for real-time / mobile devices   |\n",
    "# | **EfficientNet-B0** | \\~5.3M     | \\~0.39         | High accuracy-per-FLOP; good on small/medium datasets         |\n",
    "# | **GhostNet**        | \\~5M       | Very low       | Ultra-light, highly efficient; used in YOLOv7-Tiny            |\n",
    "# | **ShuffleNetV2**    | \\~2.3M     | Very low       | Designed for low-latency mobile inference                     |\n",
    "# | **DenseNet-121**    | \\~8M       | \\~2.9          | Dense connections; better gradient flow but slower            |\n",
    "# | **ConvNeXt-T**      | \\~28M      | \\~4.5          | Modern ConvNet with transformer-like performance              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125 0.0625 0.05\n"
     ]
    }
   ],
   "source": [
    "#i      (f,     n,      m,  args)\n",
    "#       from  number module args   args[0] 512   args[1:] rest\n",
    "#  i      f             t\n",
    "# m_.i, m_.f,        m_.type \n",
    "# c1 c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   - [-1, 1, Timm, [512, 'efficientnet_b5', True, True, 0, True]]  # Layer 0: returns list of block outputs\n",
    "#   - [0, 1, Index, [64, 2]]     # Layer 1: P3 = Block 2 output (32x32)\n",
    "#   - [0, 1, Index, [176, 3]]    # Layer 2: P4 = Block 4 output (16x16)\n",
    "#   - [0, 1, Index, [512, 4]]    # Layer 3: P5 = Block 6 output (8x8)\n",
    "#   - [-1, 1, SPPF, [512, 5]]    # Layer 4: SPPF on P5 (8x8) to enhance receptive field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !python -m build --wheel --no-isolation\n",
    "# !pip install --upgrade build wheel setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from timm) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from timm) (0.20.1+cu121)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from timm) (6.0.2)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-0.32.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from huggingface_hub->timm) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from huggingface_hub->timm) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from huggingface_hub->timm) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from torch->timm) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from torchvision->timm) (11.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\freedomkwok2022\\ml_learn\\20241127_course_01_js2024_baseline\\env\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 44.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.32.3-py3-none-any.whl (512 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Installing collected packages: safetensors, huggingface_hub, timm\n",
      "Successfully installed huggingface_hub-0.32.3 safetensors-0.5.3 timm-1.0.15\n"
     ]
    }
   ],
   "source": [
    "! pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:37:45.574748Z",
     "iopub.status.busy": "2025-05-28T20:37:45.574402Z",
     "iopub.status.idle": "2025-05-28T20:37:48.124129Z",
     "shell.execute_reply": "2025-05-28T20:37:48.122868Z",
     "shell.execute_reply.started": "2025-05-28T20:37:45.574713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 方法 A：直接指定文件路径\n",
    "#!pip install /kaggle/input/ultralytics-thop/ultralytics_thop-2.0.14-py3-none-any.whl --force-reinstall --no-index --no-deps\n",
    "!pip install /kaggle/input/ultralytics-timm/ultralytics-8.3.133-py3-none-any.whl --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:41:14.056325Z",
     "iopub.status.busy": "2025-05-28T21:41:14.055972Z",
     "iopub.status.idle": "2025-05-28T21:41:15.799699Z",
     "shell.execute_reply": "2025-05-28T21:41:15.798732Z",
     "shell.execute_reply.started": "2025-05-28T21:41:14.056294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/yolo-plus/ultralytics-8.3.133-py3-none-any.whl --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "backbone = timm.create_model(\"resnet50\", pretrained=True, features_only=True)\n",
    "feats = backbone(x)\n",
    "print([f.shape for f in feats])\n",
    "\n",
    "# [\n",
    "#   [1, 64, 112, 112],\n",
    "#   [1, 256, 56, 56],   # stage 1\n",
    "#   [1, 512, 28, 28],   # stage 2 -> P3\n",
    "#   [1, 1024, 14, 14],  # stage 3 -> P4\n",
    "#   [1, 2048, 7, 7]     # stage 4 -> P5\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:38:11.041418Z",
     "iopub.status.busy": "2025-05-28T20:38:11.041031Z",
     "iopub.status.idle": "2025-05-28T20:38:21.616292Z",
     "shell.execute_reply": "2025-05-28T20:38:21.615215Z",
     "shell.execute_reply.started": "2025-05-28T20:38:11.041385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1: torch.Size([1, 256, 64, 64])\n",
      "layer2: torch.Size([1, 512, 32, 32])\n",
      "layer3: torch.Size([1, 1024, 16, 16])\n",
      "layer4: torch.Size([1, 2048, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "model = timm.create_model('resnet50', pretrained=False)\n",
    "model.eval()\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "def save_hook(name):\n",
    "    def fn(module, input, output):\n",
    "        print(f\"{name}: {output.shape}\")\n",
    "    return fn\n",
    "\n",
    "handles = []\n",
    "for i in range(1, 5):  # layer1~layer4\n",
    "    layer = getattr(model, f'layer{i}')\n",
    "    handles.append(layer.register_forward_hook(save_hook(f'layer{i}')))\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(x)\n",
    "\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "# layer1: torch.Size([1, 256, 64, 64]) # stage 2 (C2)\n",
    "# layer2: torch.Size([1, 512, 32, 32]) # stage 3 (C3)\n",
    "# layer3: torch.Size([1, 1024, 16, 16]) # stage 4 (C4)\n",
    "# layer4: torch.Size([1, 2048, 8, 8]) # stage 5 (C5)\n",
    "\n",
    "# | YOLO Head Input | ResNet Stage | Output Shape        | Notes                   |\n",
    "# | --------------- | ------------ | ------------------- | ----------------------- |\n",
    "# | P3/8            | layer2       | `[1, 512, 32, 32]`  | Spatial downscale = 8×  | => 256 / 32 >= 8x\n",
    "# | P4/16           | layer3       | `[1, 1024, 16, 16]` | Spatial downscale = 16× | => 256 / 32 >= 16x\n",
    "# | P5/32           | layer4       | `[1, 2048, 8, 8]`   | Spatial downscale = 32× | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:38:23.206822Z",
     "iopub.status.busy": "2025-05-28T20:38:23.206551Z",
     "iopub.status.idle": "2025-05-28T20:38:23.918265Z",
     "shell.execute_reply": "2025-05-28T20:38:23.917376Z",
     "shell.execute_reply.started": "2025-05-28T20:38:23.206800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba96063d9ae54bac86988088ae9ca93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage0: torch.Size([1, 96, 64, 64])\n",
      "stage1: torch.Size([1, 192, 32, 32])\n",
      "stage2: torch.Size([1, 384, 16, 16])\n",
      "stage3: torch.Size([1, 768, 8, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Freedomkwok2022\\ML_Learn\\20241127_Course_01_JS2024_Baseline\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Freedomkwok2022\\.cache\\huggingface\\hub\\models--timm--convnext_tiny.in12k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "model = timm.create_model('convnext_tiny', features_only=True, pretrained=True)\n",
    "model.eval()\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats = model(x)\n",
    "    for i, f in enumerate(feats):\n",
    "        print(f\"stage{i}: {f.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:38:23.919839Z",
     "iopub.status.busy": "2025-05-28T20:38:23.919485Z",
     "iopub.status.idle": "2025-05-28T20:38:24.622444Z",
     "shell.execute_reply": "2025-05-28T20:38:24.621535Z",
     "shell.execute_reply.started": "2025-05-28T20:38:23.919806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "model = timm.create_model('swinv2_tiny_window8_256', pretrained=False)\n",
    "model.eval()\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "def save_hook(name):\n",
    "    def fn(module, input, output):\n",
    "        print(f\"{name}: {output.shape}\")\n",
    "    return fn\n",
    "\n",
    "handles = []\n",
    "# Swin V2 的主干是 model.layers[0], model.layers[1], model.layers[2], model.layers[3]\n",
    "for i in range(4):\n",
    "    handles.append(model.layers[i].register_forward_hook(save_hook(f'layers[{i}]')))\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(x)\n",
    "\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:43:00.968751Z",
     "iopub.status.busy": "2025-05-28T20:43:00.968405Z",
     "iopub.status.idle": "2025-05-28T20:43:01.660515Z",
     "shell.execute_reply": "2025-05-28T20:43:01.659549Z",
     "shell.execute_reply.started": "2025-05-28T20:43:00.968723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0: torch.Size([1, 24, 128, 128])\n",
      "Block 1: torch.Size([1, 40, 64, 64])\n",
      "Block 2: torch.Size([1, 64, 32, 32])\n",
      "Block 3: torch.Size([1, 128, 16, 16])\n",
      "Block 4: torch.Size([1, 176, 16, 16])\n",
      "Block 5: torch.Size([1, 304, 8, 8])\n",
      "Block 6: torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "model = timm.create_model('efficientnet_b5', pretrained=False)\n",
    "model.eval()\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# 用 hook 方式输出每个 block 的 shape\n",
    "def save_hook(name):\n",
    "    def fn(module, input, output):\n",
    "        print(f\"{name}: {output.shape}\")\n",
    "    return fn\n",
    "\n",
    "handles = []\n",
    "for i, block in enumerate(model.blocks):\n",
    "    handles.append(block.register_forward_hook(save_hook(f\"Block {i}\")))\n",
    "\n",
    "# 正确做法：直接把 x 输入整个 model\n",
    "with torch.no_grad():\n",
    "    _ = model(x)\n",
    "\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "# Block 0: torch.Size([1, 24, 128, 128])\n",
    "# Block 1: torch.Size([1, 40, 64, 64]) \n",
    "# Block 2: torch.Size([1, 64, 32, 32]) 8x\n",
    "# Block 3: torch.Size([1, 128, 16, 16])\n",
    "# Block 4: torch.Size([1, 176, 16, 16]) 16\n",
    "# Block 5: torch.Size([1, 304, 8, 8])\n",
    "# Block 6: torch.Size([1, 512, 8, 8])  32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:47:36.353872Z",
     "iopub.status.busy": "2025-05-28T20:47:36.353503Z",
     "iopub.status.idle": "2025-05-28T20:47:36.621713Z",
     "shell.execute_reply": "2025-05-28T20:47:36.620584Z",
     "shell.execute_reply.started": "2025-05-28T20:47:36.353841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "model = timm.create_model('efficientnet_b2', pretrained=False)\n",
    "model.eval()\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# 用 hook 方式输出每个 block 的 shape\n",
    "def save_hook(name):\n",
    "    def fn(module, input, output):\n",
    "        print(f\"{name}: {output.shape}\")\n",
    "    return fn\n",
    "\n",
    "handles = []\n",
    "for i, block in enumerate(model.blocks):\n",
    "    handles.append(block.register_forward_hook(save_hook(f\"Block {i}\")))\n",
    "\n",
    "# 正确做法：直接把 x 输入整个 model\n",
    "with torch.no_grad():\n",
    "    _ = model(x)\n",
    "\n",
    "for h in handles:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:50:38.858834Z",
     "iopub.status.busy": "2025-05-28T20:50:38.858493Z",
     "iopub.status.idle": "2025-05-28T20:50:39.415212Z",
     "shell.execute_reply": "2025-05-28T20:50:39.414092Z",
     "shell.execute_reply.started": "2025-05-28T20:50:38.858811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# | Stage | Layer | Operation  | Input Layer | EfficientNet Block | Channels (EffNet) | Spatial Size | YOLOv11n Layer(s) | Channels (YOLOv11n) | Notes                    |\n",
    "# | ----- | ----- | ---------- | ----------- | ------------------ | ----------------- | ------------ | ----------------- | ------------------- | ------------------------ |\n",
    "# | P3    | 1     | `Index[2]` | Layer 0     | Block 2            | 64                | 32×32        | Layer 3           | 256                 | First detect scale       |\n",
    "# | P4    | 2     | `Index[4]` | Layer 0     | Block 4            | 176               | 16×16        | Layers 4–6        | 512                 | Mid detect scale         |\n",
    "# | P5    | 3     | `Index[6]` | Layer 0     | Block 6            | 512               | 8×8          | Layers 7–9        | 1024                | Deepest feature          |\n",
    "# | P5+   | 4     | `SPPF`     | Layer 3     | —                  | 512               | 8×8          | Layer 9           | 1024                | Expanded receptive field |\n",
    "\n",
    "# | Feature          | EffNet-B5 Head                                           | YOLOv11n Head                                   | Notes                                          |\n",
    "# | ---------------- | -------------------------------------------------------- | ----------------------------------------------- | ---------------------------------------------- |\n",
    "# | **P5** (deepest) | `SPPF` output (8×8) = Layer 4                            | Layer 10: Backbone P5 (20×20 → 8×8)             | Both start from 8×8                            |\n",
    "# | → refine P5      | `C2f[512]` after concat                                  | `C3k2[1024]` after concat (Layer 22)            | YOLOv11n uses 1024, EffNet uses 512            |\n",
    "# | **P4** (middle)  | `Upsample`, concat with P4 (Layer 2 = 16×16), `C2f[176]` | `Upsample`, concat Layer 6, `C3k2[512]`         | EffNet uses 176, YOLOv11n uses 512             |\n",
    "# | → refine again   | Downsample + concat (32→16) + `C2f[176]` again           | Downsample + concat → `C3k2[512]` again         | Both re-process P4 mid-level fusion            |\n",
    "# | **P3** (shallow) | `Upsample`, concat with P3 (Layer 1 = 32×32), `C2f[64]`  | `Upsample`, concat Layer 4, `C3k2[256]`         | EffNet uses 64, YOLOv11n uses 256              |\n",
    "# | → refine again   | Downsample + concat (→16×16) + `C2f[176]`                | Downsample + concat → `C3k2[512]`               | Second P3 fusion → P4 again                    |\n",
    "# | **Detection**    | `Detect[64, 176, 512]` (layers 10,13,16)                 | `YOLOEDetect[256, 512, 1024]` (layers 16,19,22) | Channels are projected smaller in EffNet setup |\n",
    "\n",
    "\n",
    "# efficientnet_b5\n",
    "# Block 0: torch.Size([1, 24, 128, 128])\n",
    "# Block 1: torch.Size([1, 40, 64, 64])\n",
    "# Block 2: torch.Size([1, 64, 32, 32])   # Index 2 x 8\n",
    "# Block 3: torch.Size([1, 128, 16, 16])  \n",
    "# Block 4: torch.Size([1, 176, 16, 16])   # Index 4 x16\n",
    "# Block 5: torch.Size([1, 304, 8, 8])\n",
    "# Block 6: torch.Size([1, 512, 8, 8])    # Index 6 x 32\n",
    "\n",
    "# 2. 然后正常 import 并构建  \n",
    "from ultralytics import YOLO\n",
    "# 1. 你的 YAML 配置字符串\n",
    "# import timm.layers.patch_embed as _pe\n",
    "# _pe._assert = lambda cond, msg=None: None\n",
    "eff_yolo_config = \"\"\"\n",
    "nc: 1  # number of classes\n",
    "\n",
    "backbone:\n",
    "  - [-1, 1, Timm, [512, 'efficientnet_b5', True, True, 0, True]]  # Layer 0: returns list of block outputs\n",
    "  - [0, 1, Index, [64, 2]]     # Layer 1: P3 = Block 2 output (32x32)\n",
    "  - [0, 1, Index, [176, 4]]    # Layer 2: P4 = Block 4 output (16x16)\n",
    "  - [0, 1, Index, [512, 6]]    # Layer 3: P5 = Block 6 output (8x8)\n",
    "  - [-1, 1, SPPF, [512, 5]]    # Layer 4: SPPF on P5 (8x8) to enhance receptive field\n",
    "\n",
    "head:\n",
    "  # 上采样/拼接/检测头，通道数建议与 backbone 输出保持一致\n",
    "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]            # 5, SPPF上采样 (8->16)\n",
    "  - [[-1, 2], 1, Concat, [1]]                             # 6, 拼接16x16的两个特征\n",
    "  - [-1, 3, C2f, [176]]                                   # 7, 通道数和P4对齐\n",
    "\n",
    "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]            # 8, 上采样 (16->32)\n",
    "  - [[-1, 1], 1, Concat, [1]]                             # 9, 拼接32x32的两个特征\n",
    "  - [-1, 3, C2f, [64]]                                    # 10, 通道数和P3对齐\n",
    "\n",
    "  - [-1, 1, Conv, [64, 3, 2]]                             # 11, 下采样 (32->16)\n",
    "  - [[-1, 7], 1, Concat, [1]]                             # 12, 拼接16x16的两个特征\n",
    "  - [-1, 3, C2f, [176]]                                   # 13\n",
    "\n",
    "  - [-1, 1, Conv, [176, 3, 2]]                            # 14, 下采样 (16->8)\n",
    "  - [[-1, 4], 1, Concat, [1]]                             # 15, 拼接8x8的两个特征\n",
    "  - [-1, 3, C2f, [512]]                                   # 16\n",
    "\n",
    "  - [[10, 13, 16], 1, Detect, [nc]]                       # 17, 检测头, 多尺度\n",
    "\"\"\"\n",
    "\n",
    "swin_yolo_config = \"\"\"\n",
    "nc: 80 # number of classes\n",
    "backbone:\n",
    "  - [-1, 1, Timm, [352, 'efficientnet_b2', True, True, 0, True]]\n",
    "  - [0, 1, Index, [48, 2]]    # features[2]  [1, 64, 32, 32]\n",
    "  - [0, 1, Index, [120, 3]]   # features[4]  [1, 176, 16, 16]\n",
    "  - [0, 1, Index, [352, 4]]   # features[6]  [1, 512, 8, 8]\n",
    "  - [-1, 1, SPPF, [352, 5]]\n",
    "\n",
    "head:\n",
    "  # 上采样/拼接/检测头，通道数建议与 backbone 输出保持一致\n",
    "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]            # 5, SPPF上采样 (8->16)\n",
    "  - [[-1, 2], 1, Concat, [1]]                             # 6, 拼接16x16的两个特征\n",
    "  - [-1, 3, C2f, [120]]                                   # 7, 通道数和P4对齐\n",
    "\n",
    "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]            # 8, 上采样 (16->32)\n",
    "  - [[-1, 1], 1, Concat, [1]]                             # 9, 拼接32x32的两个特征\n",
    "  - [-1, 3, C2f, [48]]                                    # 10, 通道数和P3对齐\n",
    "\n",
    "  - [-1, 1, Conv, [64, 3, 2]]                             # 11, 下采样 (32->16)\n",
    "  - [[-1, 7], 1, Concat, [1]]                             # 12, 拼接16x16的两个特征\n",
    "  - [-1, 3, C2f, [120]]                                   # 13\n",
    "\n",
    "  - [-1, 1, Conv, [176, 3, 2]]                            # 14, 下采样 (16->8)\n",
    "  - [[-1, 4], 1, Concat, [1]]                             # 15, 拼接8x8的两个特征\n",
    "  - [-1, 3, C2f, [352]]                                   # 16\n",
    "\n",
    "  - [[10, 13, 16], 1, Detect, [nc]]                       # 17, 检测头, 多尺度\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 2. 写入到本地文件\n",
    "yaml_path= \"swin_yolo.yaml\"\n",
    "with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(swin_yolo_config)\n",
    "\n",
    "# 3. 直接用文件路径加载模型结构\n",
    "model = YOLO(yaml_path,verbose=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:41:23.657377Z",
     "iopub.status.busy": "2025-05-28T21:41:23.657040Z",
     "iopub.status.idle": "2025-05-28T21:41:27.652555Z",
     "shell.execute_reply": "2025-05-28T21:41:27.651857Z",
     "shell.execute_reply.started": "2025-05-28T21:41:23.657352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from ultralytics import YOLO\n",
    "swin_yolo_config = \"\"\"\n",
    "# Parameters\n",
    "nc: 80  # number of classes\n",
    "scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'\n",
    "  # [depth, width, max_channels]\n",
    "  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs\n",
    "  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs\n",
    "  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs\n",
    "  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs\n",
    "  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs\n",
    "\n",
    "# YOLOv8.0n backbone\n",
    "backbone:\n",
    "  # [from, repeats, module, args]\n",
    "  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2\n",
    "  - [-1, 1, Conv, [128, 3, 1]]  # 1\n",
    "  - [-1, 1, space_to_depth,[1]] # 2-P2/4\n",
    "  - [-1, 3, C2f, [128, True]]\n",
    "  - [-1, 1, Conv, [256, 3, 1]]  # 4\n",
    "  - [-1, 1, space_to_depth,[1]] # 5-P3/8\n",
    "  - [-1, 6, C2f, [256, True]]\n",
    "  - [-1, 1, Conv, [512, 3, 1]]  # 7\n",
    "  - [-1, 1, space_to_depth,[1]] # 8-P4/16\n",
    "  - [-1, 6, C2f, [512, True]]\n",
    "  - [-1, 1, Conv, [1024, 3, 1]]  # 10\n",
    "  - [-1, 1, space_to_depth,[1]] # 11-P5/32\n",
    "  - [-1, 3, C2f, [1024, True]]\n",
    "  - [-1, 1, SPPF, [1024, 5]]  # 13\n",
    "\n",
    "# YOLOv8.0n head\n",
    "head:\n",
    "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n",
    "  - [[-1, 9], 1, Concat, [1]]  # cat backbone P4\n",
    "  - [-1, 3, C2f, [512]]  # 16\n",
    "\n",
    "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n",
    "  - [[-1, 6], 1, Concat, [1]]  # cat backbone P3\n",
    "  - [-1, 3, C2f, [256]]  # 19 (P3/8-small)\n",
    "\n",
    "  - [-1, 1, Conv, [256, 3, 2]]\n",
    "  - [[-1, 16], 1, Concat, [1]]  # cat head P4\n",
    "  - [-1, 3, C2f, [512]]  # 22 (P4/16-medium)\n",
    "\n",
    "  - [-1, 1, Conv, [512, 3, 2]]\n",
    "  - [[-1, 13], 1, Concat, [1]]  # cat head P5\n",
    "  - [-1, 3, C2f, [1024]]  # 25 (P5/32-large)\n",
    "\n",
    "  - [[19, 22, 25], 1, Detect, [nc]]  # Detect(P3, P4, P5)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "yaml_path= \"swin_yolo.yaml\"\n",
    "with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(swin_yolo_config)\n",
    "\n",
    "# 3. 直接用文件路径加载模型结构\n",
    "model = YOLO(yaml_path,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:53:27.158946Z",
     "iopub.status.busy": "2025-05-28T21:53:27.158496Z",
     "iopub.status.idle": "2025-05-28T21:53:27.164303Z",
     "shell.execute_reply": "2025-05-28T21:53:27.163496Z",
     "shell.execute_reply.started": "2025-05-28T21:53:27.158902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "swin_yolo_config = \"\"\"\n",
    "nc: 10  # number of classes\n",
    "depth_multiple: 0.33  # scales module repeats\n",
    "width_multiple: 1.00  # scales convolution channels\n",
    "\n",
    "# YOLOv8.0n backbone\n",
    "backbone:\n",
    "  # [from, repeats, module, args]\n",
    "  - [-1, 1, Conv, [16, 3, 2]]  # 0-P1/2\n",
    "  - [-1, 1, Conv, [32, 3, 2]]  # 1-P2/4\n",
    "  - [-1, 3, C2f, [32, True]]\n",
    "  - [-1, 1, Conv, [64, 3, 2]]  # 3-P3/8\n",
    "  - [-1, 6, C2f, [64, True]]\n",
    "  - [-1, 1, Conv, [128, 3, 2]]  # 5-P4/16\n",
    "  - [-1, 6, C2f, [128, True]]\n",
    "  - [-1, 1, Conv, [256, 3, 2]]  # 7-P5/32\n",
    "  - [-1, 3, C2f, [256, True]]\n",
    "  - [-1, 1, SPPF, [256, 5]]  # 9\n",
    "\n",
    "# YOLOv8.0n head\n",
    "head:\n",
    "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n",
    "  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4\n",
    "  - [-1, 3, C2f, [128]]  # 12\n",
    "\n",
    "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n",
    "  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3\n",
    "  - [-1, 3, C2f, [64]]  # 15 (P3/8-small)\n",
    "\n",
    "  - [[15], 1, DyDetect, [nc]]  # Detect(P3, P4, P5)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# yaml_path= \"swin_yolo.yaml\"\n",
    "# with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(swin_yolo_config)\n",
    "\n",
    "# # 3. 直接用文件路径加载模型结构\n",
    "# model = YOLO(yaml_path,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:15:23.541701Z",
     "iopub.status.busy": "2025-05-28T20:15:23.541335Z",
     "iopub.status.idle": "2025-05-28T20:15:23.545225Z",
     "shell.execute_reply": "2025-05-28T20:15:23.544540Z",
     "shell.execute_reply.started": "2025-05-28T20:15:23.541672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ensure_channels_first(x):\n",
    "    # x: [B, H, W, C] or [B, C, H, W]\n",
    "    if x.dim() == 4 and x.shape[1] < 10 and x.shape[-1] > 10:\n",
    "        return x.permute(0, 3, 1, 2).contiguous()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:15:48.601712Z",
     "iopub.status.busy": "2025-05-28T20:15:48.601373Z",
     "iopub.status.idle": "2025-05-28T20:15:49.559768Z",
     "shell.execute_reply": "2025-05-28T20:15:49.559062Z",
     "shell.execute_reply.started": "2025-05-28T20:15:48.601685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = YOLO(yaml_path,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:38.344483Z",
     "iopub.status.busy": "2025-05-28T20:39:38.344118Z",
     "iopub.status.idle": "2025-05-28T20:40:30.513159Z",
     "shell.execute_reply": "2025-05-28T20:40:30.511570Z",
     "shell.execute_reply.started": "2025-05-28T20:39:38.344454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "# from ultralytics import YOLO\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define paths for Kaggle environment\n",
    "yolo_dataset_dir = \"/kaggle/input/parse-data/yolo_dataset\"\n",
    "yolo_weights_dir = \"/kaggle/working/yolo_weights\"\n",
    "yolo_pretrained_weights = \"/kaggle/input/yolo11/pytorch/default/1/yolo11n.pt\"  # Path to pre-downloaded weights\n",
    "\n",
    "# Create weights directory if it doesn't exist\n",
    "os.makedirs(yolo_weights_dir, exist_ok=True)\n",
    "\n",
    "def fix_yaml_paths(yaml_path):\n",
    "    \"\"\"\n",
    "    Fix the paths in the YAML file to match the actual Kaggle directories\n",
    "    \n",
    "    Args:\n",
    "        yaml_path (str): Path to the original dataset YAML file\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the fixed YAML file\n",
    "    \"\"\"\n",
    "    print(f\"Fixing YAML paths in {yaml_path}\")\n",
    "    \n",
    "    # Read the original YAML\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        yaml_data = yaml.safe_load(f)\n",
    "    \n",
    "    # Update paths to use actual dataset location\n",
    "    if 'path' in yaml_data:\n",
    "        yaml_data['path'] = yolo_dataset_dir\n",
    "    \n",
    "    # Create a new fixed YAML in the working directory\n",
    "    fixed_yaml_path = \"/kaggle/working/fixed_dataset.yaml\"\n",
    "    with open(fixed_yaml_path, 'w') as f:\n",
    "        yaml.dump(yaml_data, f)\n",
    "    \n",
    "    print(f\"Created fixed YAML at {fixed_yaml_path} with path: {yaml_data.get('path')}\")\n",
    "    return fixed_yaml_path\n",
    "\n",
    "def plot_dfl_loss_curve(run_dir):\n",
    "    \"\"\"\n",
    "    Plot the DFL loss curves for train and validation, marking the best model\n",
    "    \n",
    "    Args:\n",
    "        run_dir (str): Directory where the training results are stored\n",
    "    \"\"\"\n",
    "    # Path to the results CSV file\n",
    "    results_csv = os.path.join(run_dir, 'results.csv')\n",
    "    \n",
    "    if not os.path.exists(results_csv):\n",
    "        print(f\"Results file not found at {results_csv}\")\n",
    "        return\n",
    "    \n",
    "    # Read results CSV\n",
    "    results_df = pd.read_csv(results_csv)\n",
    "    \n",
    "    # Check if DFL loss columns exist\n",
    "    train_dfl_col = [col for col in results_df.columns if 'train/dfl_loss' in col]\n",
    "    val_dfl_col = [col for col in results_df.columns if 'val/dfl_loss' in col]\n",
    "    \n",
    "    if not train_dfl_col or not val_dfl_col:\n",
    "        print(\"DFL loss columns not found in results CSV\")\n",
    "        print(f\"Available columns: {results_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    train_dfl_col = train_dfl_col[0]\n",
    "    val_dfl_col = val_dfl_col[0]\n",
    "    \n",
    "    # Find the epoch with the best validation loss\n",
    "    best_epoch = results_df[val_dfl_col].idxmin()\n",
    "    best_val_loss = results_df.loc[best_epoch, val_dfl_col]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.plot(results_df['epoch'], results_df[train_dfl_col], label='Train DFL Loss')\n",
    "    plt.plot(results_df['epoch'], results_df[val_dfl_col], label='Validation DFL Loss')\n",
    "    \n",
    "    # Mark the best model with a vertical line\n",
    "    plt.axvline(x=results_df.loc[best_epoch, 'epoch'], color='r', linestyle='--', \n",
    "                label=f'Best Model (Epoch {int(results_df.loc[best_epoch, \"epoch\"])}, Val Loss: {best_val_loss:.4f})')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('DFL Loss')\n",
    "    plt.title('Training and Validation DFL Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save the plot in the same directory as weights\n",
    "    plot_path = os.path.join(run_dir, 'dfl_loss_curve.png')\n",
    "    plt.savefig(plot_path)\n",
    "    \n",
    "    # Also save it to the working directory for easier access\n",
    "    plt.savefig(os.path.join('/kaggle/working', 'dfl_loss_curve.png'))\n",
    "    \n",
    "    print(f\"Loss curve saved to {plot_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Return the best epoch info\n",
    "    return best_epoch, best_val_loss\n",
    "\n",
    "def train_yolo_model(yaml_path, pretrained_weights_path, epochs=30, batch_size=4, img_size=640):\n",
    "    \"\"\"\n",
    "    Train a YOLO model on the prepared dataset\n",
    "    \n",
    "    Args:\n",
    "        yaml_path (str): Path to the dataset YAML file\n",
    "        pretrained_weights_path (str): Path to pre-downloaded weights file\n",
    "        epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "        img_size (int): Image size for training\n",
    "    \"\"\"\n",
    "    print(f\"Loading pre-trained weights from: {pretrained_weights_path}\")\n",
    "    \n",
    "    # Load a pre-trained YOLOv8 model\n",
    "    #model = YOLO(pretrained_weights_path)\n",
    "    model = YOLO(\"swin_yolo.yaml\")\n",
    "    # Train the model with early stopping\n",
    "    results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=img_size,\n",
    "        project=yolo_weights_dir,\n",
    "        name='motor_detector',\n",
    "        exist_ok=True,\n",
    "        patience=5,              # Early stopping if no improvement for 5 epochs\n",
    "        save_period=5,           # Save checkpoints every 5 epochs\n",
    "        val=True,                # Ensure validation is performed\n",
    "        verbose=True             # Show detailed output during training\n",
    "    )\n",
    "    \n",
    "    # Get the path to the run directory\n",
    "    run_dir = os.path.join(yolo_weights_dir, 'motor_detector')\n",
    "    \n",
    "    # Plot and save the loss curve\n",
    "    best_epoch_info = plot_dfl_loss_curve(run_dir)\n",
    "    \n",
    "    if best_epoch_info:\n",
    "        best_epoch, best_val_loss = best_epoch_info\n",
    "        print(f\"\\nBest model found at epoch {best_epoch} with validation DFL loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "def predict_on_samples(model, num_samples=4):\n",
    "    \"\"\"\n",
    "    Run predictions on random validation samples and display results\n",
    "    \n",
    "    Args:\n",
    "        model: Trained YOLO model\n",
    "        num_samples (int): Number of random samples to test\n",
    "    \"\"\"\n",
    "    # Get validation images\n",
    "    val_dir = os.path.join(yolo_dataset_dir, 'images', 'val')\n",
    "    if not os.path.exists(val_dir):\n",
    "        print(f\"Validation directory not found at {val_dir}\")\n",
    "        # Try train directory instead if val doesn't exist\n",
    "        val_dir = os.path.join(yolo_dataset_dir, 'images', 'train')\n",
    "        print(f\"Using train directory for predictions instead: {val_dir}\")\n",
    "        \n",
    "    if not os.path.exists(val_dir):\n",
    "        print(\"No images directory found for predictions\")\n",
    "        return\n",
    "    \n",
    "    val_images = os.listdir(val_dir)\n",
    "    \n",
    "    if len(val_images) == 0:\n",
    "        print(\"No images found for prediction\")\n",
    "        return\n",
    "    \n",
    "    # Select random samples\n",
    "    num_samples = min(num_samples, len(val_images))\n",
    "    samples = random.sample(val_images, num_samples)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img_file in enumerate(samples):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        img_path = os.path.join(val_dir, img_file)\n",
    "        \n",
    "        # Run prediction\n",
    "        results = model.predict(img_path, conf=0.25)[0]\n",
    "        \n",
    "        # Load and display the image\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(np.array(img), cmap='gray')\n",
    "        \n",
    "        # Draw ground truth box if available (from filename)\n",
    "        try:\n",
    "            # This assumes your filenames contain coordinates in a specific format\n",
    "            parts = img_file.split('_')\n",
    "            y_part = [p for p in parts if p.startswith('y')]\n",
    "            x_part = [p for p in parts if p.startswith('x')]\n",
    "            \n",
    "            if y_part and x_part:\n",
    "                y_gt = int(y_part[0][1:])\n",
    "                x_gt = int(x_part[0][1:].split('.')[0])\n",
    "                \n",
    "                box_size = 24\n",
    "                rect_gt = Rectangle((x_gt - box_size//2, y_gt - box_size//2), \n",
    "                              box_size, box_size, \n",
    "                              linewidth=1, edgecolor='g', facecolor='none')\n",
    "                axes[i].add_patch(rect_gt)\n",
    "        except:\n",
    "            pass  # Skip ground truth if parsing fails\n",
    "        \n",
    "        # Draw predicted boxes (red)\n",
    "        if len(results.boxes) > 0:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            confs = results.boxes.conf.cpu().numpy()\n",
    "            \n",
    "            for box, conf in zip(boxes, confs):\n",
    "                x1, y1, x2, y2 = box\n",
    "                rect_pred = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                     linewidth=1, edgecolor='r', facecolor='none')\n",
    "                axes[i].add_patch(rect_pred)\n",
    "                axes[i].text(x1, y1-5, f'{conf:.2f}', color='red')\n",
    "        \n",
    "        axes[i].set_title(f\"Image: {img_file}\\nGround Truth (green) vs Prediction (red)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the predictions plot\n",
    "    plt.savefig(os.path.join('/kaggle/working', 'predictions.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Check and create a dataset YAML if needed\n",
    "def prepare_dataset():\n",
    "    \"\"\"\n",
    "    Check if dataset exists and create a proper YAML if needed\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the YAML file to use for training\n",
    "    \"\"\"\n",
    "    # Check if images exist\n",
    "    train_images_dir = os.path.join(yolo_dataset_dir, 'images', 'train')\n",
    "    val_images_dir = os.path.join(yolo_dataset_dir, 'images', 'val')\n",
    "    train_labels_dir = os.path.join(yolo_dataset_dir, 'labels', 'train')\n",
    "    val_labels_dir = os.path.join(yolo_dataset_dir, 'labels', 'val')\n",
    "    \n",
    "    # Print directory existence status\n",
    "    print(f\"Directory status:\")\n",
    "    print(f\"- Train images dir exists: {os.path.exists(train_images_dir)}\")\n",
    "    print(f\"- Val images dir exists: {os.path.exists(val_images_dir)}\")\n",
    "    print(f\"- Train labels dir exists: {os.path.exists(train_labels_dir)}\")\n",
    "    print(f\"- Val labels dir exists: {os.path.exists(val_labels_dir)}\")\n",
    "    \n",
    "    # Check for original YAML file\n",
    "    original_yaml_path = os.path.join(yolo_dataset_dir, 'dataset.yaml')\n",
    "    \n",
    "    if os.path.exists(original_yaml_path):\n",
    "        print(f\"Found original dataset.yaml at {original_yaml_path}\")\n",
    "        # Fix the paths in the YAML\n",
    "        return fix_yaml_paths(original_yaml_path)\n",
    "    else:\n",
    "        print(f\"Original dataset.yaml not found, creating a new one\")\n",
    "        \n",
    "        # Create a new YAML file\n",
    "        yaml_data = {\n",
    "            'path': yolo_dataset_dir,\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/train' if not os.path.exists(val_images_dir) else 'images/val',\n",
    "            'names': {0: 'motor'}\n",
    "        }\n",
    "        \n",
    "        new_yaml_path = \"/kaggle/working/dataset.yaml\"\n",
    "        with open(new_yaml_path, 'w') as f:\n",
    "            yaml.dump(yaml_data, f)\n",
    "            \n",
    "        print(f\"Created new YAML at {new_yaml_path}\")\n",
    "        return new_yaml_path\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"Starting YOLO training process...\")\n",
    "    \n",
    "    # Prepare dataset and get YAML path\n",
    "    yaml_path = prepare_dataset()\n",
    "    print(f\"Using YAML file: {yaml_path}\")\n",
    "    \n",
    "    # Print YAML file contents\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        yaml_content = f.read()\n",
    "    print(f\"YAML file contents:\\n{yaml_content}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting YOLO training...\")\n",
    "    model, results = train_yolo_model(\n",
    "        yaml_path,\n",
    "        pretrained_weights_path=yolo_pretrained_weights,\n",
    "        epochs=30  # Using 30 epochs instead of 100 for faster training\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    \n",
    "    # Run predictions\n",
    "    print(\"\\nRunning predictions on sample images...\")\n",
    "    predict_on_samples(model, num_samples=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11294684,
     "isSourceIdPinned": false,
     "sourceId": 91249,
     "sourceType": "competition"
    },
    {
     "datasetId": 7538808,
     "sourceId": 11986142,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7539261,
     "sourceId": 11986780,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7539377,
     "sourceId": 11986945,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7540294,
     "sourceId": 11988291,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7540310,
     "sourceId": 11988315,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 226368929,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 141013,
     "modelInstanceId": 117776,
     "sourceId": 139093,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 141350,
     "modelInstanceId": 118113,
     "sourceId": 139474,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
